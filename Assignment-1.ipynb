{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f938c49",
   "metadata": {},
   "source": [
    "**Problem 1 [10] Pick-and-Place Robot:** Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the learning agent will have to control the motors directly and obtain feedback about the current positions and velocities of the mechanical linkages. Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025e066",
   "metadata": {},
   "source": [
    "## Problem 1: Pick-and-Place Robot as an MDP\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Modeling pick-and-place robot problem as a Markov Decision Process (MDP).\n",
    "\n",
    "An MDP is defined as:\n",
    "\n",
    "(S, A, P, R)\n",
    "\n",
    "Where:\n",
    "- S = States  \n",
    "- A = Actions  \n",
    "- P = Transition probability  \n",
    "- R = Reward function  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. States (S)\n",
    "\n",
    "The state should describe the current condition of the robot.\n",
    "\n",
    "As per the question the agent receives feedback about positions and velocities, we define the state as:\n",
    "s_t = (q_t, v_t, g_t)\n",
    "\n",
    "Where:\n",
    "- q_t = joint positions  \n",
    "- v_t = joint velocities  \n",
    "- g_t = gripper status (open or closed)\n",
    "\n",
    "Reason:\n",
    "The robot’s next movement depends only on its current position, velocity, and gripper status.  \n",
    "This satisfies the Markov property (future depends only on present state and action).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Actions (A)\n",
    "\n",
    "The agent controls the motors directly.\n",
    "\n",
    "So the actions are motor torque commands:\n",
    "\n",
    "a_t = (tau_1, tau_2, ..., tau_n)\n",
    "\n",
    "Where:\n",
    "- tau_i = torque applied to joint i\n",
    "\n",
    "Reason:\n",
    "Motor torque control allows smooth and precise movements.  \n",
    "This is necessary for learning smooth robot motion.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transition Probability (P)\n",
    "\n",
    "The transition function is:\n",
    "\n",
    "P(s' | s, a)\n",
    "\n",
    "This means:\n",
    "Given the current state and action, what is the probability of moving to the next state?\n",
    "\n",
    "In this robot system, transitions depend on:\n",
    "- Robot mechanics (physics)\n",
    "- Motor commands\n",
    "- Small noise in sensors or motors\n",
    "\n",
    "Because of noise, the system can be slightly stochastic.\n",
    "\n",
    "Also:\n",
    "\n",
    "The sum of all transition probabilities must equal 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reward Function (R)\n",
    "\n",
    "The question says we want movements that are fast and smooth.\n",
    "\n",
    "So the reward should:\n",
    "\n",
    "- Give a large positive reward when the object is successfully placed.\n",
    "- Penalize large velocities (to encourage smooth motion).\n",
    "- Penalize large torques (to avoid jerky movement).\n",
    "- Add a small time penalty (to encourage faster completion).\n",
    "\n",
    "Example reward:\n",
    "\n",
    "R = +Goal_Reward \n",
    "    - alpha * (velocity^2) \n",
    "    - beta * (torque^2) \n",
    "    - time_penalty\n",
    "\n",
    "Reason:\n",
    "This reward structure helps the robot learn:\n",
    "- Fast movement  \n",
    "- Smooth motion  \n",
    "- Energy-efficient control  \n",
    "- Successful task completion  \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The pick-and-place robot can be modeled as an MDP where:\n",
    "\n",
    "- States represent joint positions, velocities, and gripper status.\n",
    "- Actions are motor torque commands.\n",
    "- Transitions follow robot dynamics.\n",
    "- Rewards encourage fast, smooth, and successful movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7da603",
   "metadata": {},
   "source": [
    "## Problem 2: 2x2 Gridworld - Value Iteration (2 Iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c3659",
   "metadata": {},
   "source": [
    "**Environment Details:**\n",
    "2x2 Gridworld – Value Iteration\n",
    "\n",
    "State Space: s1, s2, s3, s4  \n",
    "Action Space: up, down, left, right  \n",
    "\n",
    "Rewards:\n",
    "R(s1)=5\n",
    "R(s2)=10\n",
    "R(s3)=1\n",
    "R(s4)=2\n",
    "\n",
    "Discount Factor: γ = 0.9\n",
    "\n",
    "Value Iteration Formula:\n",
    "\n",
    "V_{k+1}(s) = max_a [ R(s) + γ V_k(s') ]\n",
    "\n",
    "Transitions are deterministic.\n",
    "If action hits wall → agent stays in same state.\n",
    "Initial values: V0(s) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f14624",
   "metadata": {},
   "source": [
    "**Grid Layout:**    \n",
    "s1   s2     \n",
    "s3   s4     \n",
    "\n",
    "**Coordinates:**        \n",
    "(0,0) = s1      \n",
    "(0,1) = s2      \n",
    "(1,0) = s3      \n",
    "(1,1) = s4      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937edd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "import copy\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldValueIteration:\n",
    "    \n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Grid size\n",
    "        self.rows = 2\n",
    "        self.cols = 2\n",
    "        \n",
    "        # Rewards\n",
    "        self.rewards = np.array([[5, 10],\n",
    "                                 [1,  2]])\n",
    "        \n",
    "        # Initialize Value Function V0 = 0\n",
    "        self.V = np.zeros((self.rows, self.cols))\n",
    "        \n",
    "        # Actions: (row_change, col_change)\n",
    "        self.actions = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"left\": (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "    \n",
    "    def is_valid(self, r, c):\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "    \n",
    "    def get_next_state(self, r, c, action):\n",
    "        dr, dc = self.actions[action]\n",
    "        new_r, new_c = r + dr, c + dc\n",
    "        \n",
    "        if self.is_valid(new_r, new_c):\n",
    "            return new_r, new_c\n",
    "        else:\n",
    "            return r, c  # stay if wall\n",
    "    \n",
    "    def iterate(self, iteration_number):\n",
    "        new_V = np.zeros_like(self.V)\n",
    "        \n",
    "        print(f\"\\n========== Iteration {iteration_number} ==========\")\n",
    "        \n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                \n",
    "                action_values = []\n",
    "                \n",
    "                print(f\"\\nState ({r},{c})  Reward = {self.rewards[r,c]}\")\n",
    "                \n",
    "                for action in self.actions:\n",
    "                    next_r, next_c = self.get_next_state(r, c, action)\n",
    "                    \n",
    "                    value = self.rewards[r,c] + \\\n",
    "                            self.gamma * self.V[next_r, next_c]\n",
    "                    \n",
    "                    action_values.append(value)\n",
    "                    \n",
    "                    print(f\" Action: {action} -> Next: ({next_r},{next_c}) \"\n",
    "                          f\" | Value = {self.rewards[r,c]} + \"\n",
    "                          f\"{self.gamma}*{self.V[next_r,next_c]:.2f} \"\n",
    "                          f\"= {value:.2f}\")\n",
    "                \n",
    "                new_V[r,c] = max(action_values)\n",
    "                \n",
    "                print(f\" Updated V({r},{c}) = {new_V[r,c]:.2f}\")\n",
    "        \n",
    "        self.V = new_V\n",
    "        print(\"\\nUpdated Value Function:\")\n",
    "        print(self.V)\n",
    "    \n",
    "    def run(self, num_iterations=2):\n",
    "        print(\"Initial Value Function V0:\")\n",
    "        print(self.V)\n",
    "        \n",
    "        for i in range(1, num_iterations+1):\n",
    "            self.iterate(i)\n",
    "\n",
    "# Run\n",
    "grid = GridWorldValueIteration(gamma=0.9)\n",
    "grid.run(num_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89e3d7",
   "metadata": {},
   "source": [
    "Since initial V0 = 0 for all states:        \n",
    "\n",
    "V1(s) = R(s) + γ * 0                \n",
    "\n",
    "Therefore:\n",
    "\n",
    "V1(s1) = 5      \n",
    "V1(s2) = 10     \n",
    "V1(s3) = 1      \n",
    "V1(s4) = 2      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe317e9",
   "metadata": {},
   "source": [
    "Now we use V1 values.\n",
    "\n",
    "Example for s1: \n",
    "\n",
    "Possible next states:   \n",
    "Up -> s1    \n",
    "Left -> s1  \n",
    "Right -> s2 \n",
    "Down -> s3  \n",
    "\n",
    "Compute:    \n",
    "\n",
    "5 + 0.9 * 5 = 9.5     \n",
    "5 + 0.9 * 10 = 14     \n",
    "5 + 0.9 * 1 = 5.9     \n",
    "\n",
    "Take maximum:   \n",
    "\n",
    "V2(s1) = 14 \n",
    "\n",
    "Same process is repeated for all states.    \n",
    "\n",
    "Final V2:   \n",
    "\n",
    "[[14, 19],  \n",
    " [5.5, 11]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05288c2",
   "metadata": {},
   "source": [
    "Iteration 1:        \n",
    "[[ 5. 10.]          \n",
    " [ 1.  2.]]     \n",
    "\n",
    "Iteration 2:        \n",
    "[[14. 19.]      \n",
    " [ 5.5 11.]]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccef8c3",
   "metadata": {},
   "source": [
    "## Problem 3 – 5x5 Gridworld using Value Iteration\n",
    "\n",
    "In this problem, we model a 5x5 Gridworld as a Markov Decision Process (MDP).\n",
    "\n",
    "### Environment Setup:\n",
    "# 5x5 Gridworld – Value Iteration   \n",
    "**State Types:**        \n",
    "- Goal State (Terminal) = +10 reward        \n",
    "- Grey States = -5 reward       \n",
    "- Regular States = -1 reward        \n",
    "\n",
    "**Grey States:**        \n",
    "(0,4)       \n",
    "(1,2)       \n",
    "(3,0)       \n",
    "\n",
    "**Goal State:**     \n",
    "(4,4)       \n",
    "\n",
    "**Discount Factor:** gamma = 0.9        \n",
    "\n",
    "**Bellman Optimality Equation:**        \n",
    "\n",
    "V(s) = max_a [ R(s) + gamma * V(s') ]       \n",
    "\n",
    "If action hits wall → stay in same state.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: 5x5 Gridworld MDP\n",
    "\n",
    "class GridWorld5x5:\n",
    "\n",
    "    def __init__(self, gamma=0.9, log_folder=\"log\"):\n",
    "\n",
    "        self.rows = 5\n",
    "        self.cols = 5\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.goal_state = (4, 4)\n",
    "        self.grey_states = [(0, 4), (1, 2), (3, 0)]\n",
    "\n",
    "        self.rewards = self.create_reward_matrix()\n",
    "\n",
    "        self.actions = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"left\": (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "\n",
    "        # -------- LOG FOLDER SETUP --------\n",
    "        self.log_folder = log_folder\n",
    "        os.makedirs(self.log_folder, exist_ok=True)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Create reward matrix\n",
    "    # ---------------------------------------------------\n",
    "    def create_reward_matrix(self):\n",
    "        R = np.full((self.rows, self.cols), -1)\n",
    "\n",
    "        for s in self.grey_states:\n",
    "            R[s] = -5\n",
    "\n",
    "        R[self.goal_state] = 10\n",
    "        return R\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def is_valid(self, r, c):\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def get_next_state(self, r, c, action):\n",
    "        dr, dc = self.actions[action]\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        if self.is_valid(nr, nc):\n",
    "            return nr, nc\n",
    "        return r, c\n",
    "\n",
    "    # ===================================================\n",
    "    # STANDARD VALUE ITERATION WITH LOGGING\n",
    "    # ===================================================\n",
    "    def value_iteration_standard(self, theta=0.001, log_file=\"problem3_iteration_log.txt\"):\n",
    "\n",
    "        V = np.zeros((self.rows, self.cols))\n",
    "        iterations = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        full_log_path = os.path.join(self.log_folder, log_file)\n",
    "        with open(full_log_path, \"w\") as f:\n",
    "            f.write(\"===== STANDARD VALUE ITERATION =====\\n\")\n",
    "\n",
    "            while True:\n",
    "                delta = 0\n",
    "                new_V = np.copy(V)\n",
    "                iterations += 1\n",
    "\n",
    "                f.write(f\"\\nIteration {iterations}\\n\")\n",
    "\n",
    "                for r in range(self.rows):\n",
    "                    for c in range(self.cols):\n",
    "\n",
    "                        if (r, c) == self.goal_state:\n",
    "                            new_V[r, c] = self.rewards[r, c]\n",
    "                            continue\n",
    "\n",
    "                        action_values = []\n",
    "\n",
    "                        for action in self.actions:\n",
    "                            nr, nc = self.get_next_state(r, c, action)\n",
    "                            value = self.rewards[r, c] + \\\n",
    "                                    self.gamma * V[nr, nc]\n",
    "                            action_values.append(value)\n",
    "\n",
    "                        new_V[r, c] = max(action_values)\n",
    "                        delta = max(delta, abs(new_V[r, c] - V[r, c]))\n",
    "\n",
    "                V = new_V\n",
    "\n",
    "                # Log matrix\n",
    "                for row in V:\n",
    "                    f.write(\" \".join(f\"{v:7.2f}\" for v in row) + \"\\n\")\n",
    "\n",
    "                if delta < theta:\n",
    "                    f.write(\"\\nConverged.\\n\")\n",
    "                    break\n",
    "\n",
    "        end_time = time.time()\n",
    "        return V, iterations, end_time - start_time\n",
    "\n",
    "    # ===================================================\n",
    "    # IN-PLACE VALUE ITERATION\n",
    "    # ===================================================\n",
    "    def value_iteration_inplace(self, theta=0.001):\n",
    "\n",
    "        V = np.zeros((self.rows, self.cols))\n",
    "        iterations = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "            iterations += 1\n",
    "\n",
    "            for r in range(self.rows):\n",
    "                for c in range(self.cols):\n",
    "\n",
    "                    if (r, c) == self.goal_state:\n",
    "                        V[r, c] = self.rewards[r, c]\n",
    "                        continue\n",
    "\n",
    "                    action_values = []\n",
    "\n",
    "                    for action in self.actions:\n",
    "                        nr, nc = self.get_next_state(r, c, action)\n",
    "                        value = self.rewards[r, c] + \\\n",
    "                                self.gamma * V[nr, nc]\n",
    "                        action_values.append(value)\n",
    "\n",
    "                    new_value = max(action_values)\n",
    "                    delta = max(delta, abs(new_value - V[r, c]))\n",
    "                    V[r, c] = new_value\n",
    "\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        end_time = time.time()\n",
    "        return V, iterations, end_time - start_time\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Extract policy\n",
    "    # ---------------------------------------------------\n",
    "    def extract_policy(self, V):\n",
    "        policy = np.empty((self.rows, self.cols), dtype=object)\n",
    "\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "\n",
    "                if (r, c) == self.goal_state:\n",
    "                    policy[r, c] = \"G\"\n",
    "                    continue\n",
    "\n",
    "                best_action = None\n",
    "                best_value = -float(\"inf\")\n",
    "\n",
    "                for action in self.actions:\n",
    "                    nr, nc = self.get_next_state(r, c, action)\n",
    "                    value = self.rewards[r, c] + \\\n",
    "                            self.gamma * V[nr, nc]\n",
    "\n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = action\n",
    "\n",
    "                policy[r, c] = best_action\n",
    "\n",
    "        return policy\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Log Policy Matrix + Agent Movement\n",
    "    # ---------------------------------------------------\n",
    "    def log_policy_and_agent(self, policy, V,\n",
    "                             log_file=\"problem3_agent_log.txt\",\n",
    "                             start_state=(0, 0)):\n",
    "\n",
    "        full_log_path = os.path.join(self.log_folder, log_file)\n",
    "        with open(full_log_path, \"a\") as f:\n",
    "\n",
    "            f.write(\"\\n\\n===== FINAL OPTIMAL VALUE FUNCTION =====\\n\")\n",
    "            for row in V:\n",
    "                f.write(\" \".join(f\"{v:7.2f}\" for v in row) + \"\\n\")\n",
    "\n",
    "            f.write(\"\\n===== OPTIMAL POLICY MATRIX =====\\n\")\n",
    "            for r in range(self.rows):\n",
    "                row_policy = []\n",
    "                for c in range(self.cols):\n",
    "                    if (r, c) in self.grey_states:\n",
    "                        row_policy.append(\"X\")\n",
    "                    elif (r, c) == self.goal_state:\n",
    "                        row_policy.append(\"G\")\n",
    "                    else:\n",
    "                        row_policy.append(policy[r, c][0].upper())\n",
    "                f.write(\" \".join(row_policy) + \"\\n\")\n",
    "\n",
    "            # Agent movement\n",
    "            f.write(\"\\n===== AGENT MOVEMENT =====\\n\")\n",
    "\n",
    "            current_state = start_state\n",
    "            step = 0\n",
    "\n",
    "            while current_state != self.goal_state:\n",
    "                step += 1\n",
    "                r, c = current_state\n",
    "\n",
    "                grid_display = [[\".\" for _ in range(self.cols)]\n",
    "                                for _ in range(self.rows)]\n",
    "\n",
    "                for g in self.grey_states:\n",
    "                    grid_display[g[0]][g[1]] = \"X\"\n",
    "\n",
    "                grid_display[self.goal_state[0]][self.goal_state[1]] = \"G\"\n",
    "                grid_display[r][c] = \"A\"\n",
    "\n",
    "                f.write(f\"\\nStep {step}\\n\")\n",
    "                for row in grid_display:\n",
    "                    f.write(\" \".join(row) + \"\\n\")\n",
    "\n",
    "                action = policy[r, c]\n",
    "                current_state = self.get_next_state(r, c, action)\n",
    "\n",
    "            f.write(\"\\nGoal Reached.\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Grid Plot Function\n",
    "# ---------------------------------------------------\n",
    "def draw_grid(grid_obj, title, values=None, policy=None):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for r in range(grid_obj.rows):\n",
    "        for c in range(grid_obj.cols):\n",
    "\n",
    "            color = \"white\"\n",
    "\n",
    "            if (r, c) in grid_obj.grey_states:\n",
    "                color = \"darkgrey\"\n",
    "            elif (r, c) == grid_obj.goal_state:\n",
    "                color = \"lightgreen\"\n",
    "\n",
    "            rect = patches.Rectangle((c, grid_obj.rows - 1 - r), 1, 1,\n",
    "                                     edgecolor='black',\n",
    "                                     facecolor=color)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            if values is not None:\n",
    "                ax.text(c + 0.5, grid_obj.rows - 1 - r + 0.5,\n",
    "                        round(values[r, c], 2),\n",
    "                        ha='center', va='center')\n",
    "\n",
    "            if policy is not None:\n",
    "                if (r, c) not in grid_obj.grey_states and \\\n",
    "                   (r, c) != grid_obj.goal_state:\n",
    "\n",
    "                    arrow_map = {\n",
    "                        \"up\": (0, 0.3),\n",
    "                        \"down\": (0, -0.3),\n",
    "                        \"left\": (-0.3, 0),\n",
    "                        \"right\": (0.3, 0)\n",
    "                    }\n",
    "                    dx, dy = arrow_map[policy[r, c]]\n",
    "                    ax.arrow(c + 0.5, grid_obj.rows - 1 - r + 0.5,\n",
    "                             dx, dy,\n",
    "                             head_width=0.15,\n",
    "                             length_includes_head=True)\n",
    "\n",
    "    ax.set_xlim(0, grid_obj.cols)\n",
    "    ax.set_ylim(0, grid_obj.rows)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# RUN EVERYTHING\n",
    "# ===================================================\n",
    "\n",
    "grid = GridWorld5x5()\n",
    "\n",
    "# Standard VI (with logging)\n",
    "V_standard, iter_std, time_std = grid.value_iteration_standard()\n",
    "\n",
    "# In-place VI\n",
    "V_inplace, iter_in, time_in = grid.value_iteration_inplace()\n",
    "\n",
    "# Extract policy\n",
    "pi_standard = grid.extract_policy(V_standard)\n",
    "\n",
    "# Log policy + agent movement into same file\n",
    "grid.log_policy_and_agent(pi_standard, V_standard)\n",
    "\n",
    "# Display figures\n",
    "draw_grid(grid, \"Initial Grid\")\n",
    "draw_grid(grid, \"Optimal State Values V*\", values=V_standard)\n",
    "draw_grid(grid, \"Optimal Policy π*\", policy=pi_standard)\n",
    "\n",
    "print(\"\\nStandard Iterations:\", iter_std)\n",
    "print(\"In-place Iterations:\", iter_in)\n",
    "print(\"Standard Time:\", round(time_std, 6))\n",
    "print(\"In-place Time:\", round(time_in, 6))\n",
    "print(\"Same V*?\", np.allclose(V_standard, V_inplace))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb41e54",
   "metadata": {},
   "source": [
    "5 by 5 Gridworld using Value Iteration.\n",
    "\n",
    "Normal states give −1 reward, grey states give −5, and the goal gives +10.  \n",
    "The agent can move in four directions and stays in place if it hits a wall.     \n",
    "\n",
    "Strted with all values equal to zero and repeatedly updated them using the Bellman equation until they stopped changing.     \n",
    "\n",
    "After convergence, I selected the best action in each state to get the optimal policy.      \n",
    "\n",
    "The program shows four grids: the initial grid, reward grid, optimal values, and optimal policy.        \n",
    "\n",
    "Finally, I simulated the agent, and it follows the arrows to reach the goal     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ce771",
   "metadata": {},
   "source": [
    "## Problem 4 [35] Problem Statement Off-policy Monte Carlo with Importance Sampling: We will use the same environment, states, actions, and rewards in Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295963f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class GridWorld5x5MonteCarlo:\n",
    "    \"\"\"\n",
    "    5x5 Gridworld for Off-Policy Monte Carlo with Importance Sampling\n",
    "\n",
    "    State Types:\n",
    "        Regular state = -1 reward\n",
    "        Grey state    = -5 reward\n",
    "        Goal state    = +10 reward\n",
    "\n",
    "    Behavior policy: random\n",
    "    Target policy: greedy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, discount_factor=0.9, log_folder=\"log\"):\n",
    "\n",
    "        self.rows = 5\n",
    "        self.cols = 5\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "        self.goal_state = (4, 4)\n",
    "        self.grey_states = [(0, 4), (1, 2), (3, 0)]\n",
    "\n",
    "        self.action_space = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "        self.reward_matrix = self._create_reward_matrix()\n",
    "\n",
    "        # ---------- LOG FOLDER ----------\n",
    "        self.log_folder = log_folder\n",
    "        os.makedirs(self.log_folder, exist_ok=True)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Create reward matrix\n",
    "    # ---------------------------------------------------\n",
    "    def _create_reward_matrix(self):\n",
    "\n",
    "        rewards = np.full((self.rows, self.cols), -1)\n",
    "\n",
    "        for state in self.grey_states:\n",
    "            rewards[state] = -5\n",
    "\n",
    "        rewards[self.goal_state] = 10\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Check if state is valid\n",
    "    # ---------------------------------------------------\n",
    "    def _is_valid_state(self, row, col):\n",
    "        return 0 <= row < self.rows and 0 <= col < self.cols\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Deterministic transition function\n",
    "    # ---------------------------------------------------\n",
    "    def _get_next_state(self, current_state, action):\n",
    "\n",
    "        row, col = current_state\n",
    "\n",
    "        action_moves = {\n",
    "            \"up\": (-1, 0),\n",
    "            \"down\": (1, 0),\n",
    "            \"left\": (0, -1),\n",
    "            \"right\": (0, 1)\n",
    "        }\n",
    "\n",
    "        delta_row, delta_col = action_moves[action]\n",
    "        next_row = row + delta_row\n",
    "        next_col = col + delta_col\n",
    "\n",
    "        if self._is_valid_state(next_row, next_col):\n",
    "            return (next_row, next_col)\n",
    "\n",
    "        return current_state\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Behavior Policy: Uniform Random\n",
    "    # ---------------------------------------------------\n",
    "    def behavior_policy(self, state):\n",
    "        return random.choice(self.action_space)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Generate one episode using behavior policy\n",
    "    # ---------------------------------------------------\n",
    "    def generate_episode(self, max_steps=100,\n",
    "                         path_log_file=\"problem4_agent_path_log.txt\"):\n",
    "\n",
    "        episode = []\n",
    "        current_state = (0, 0)\n",
    "\n",
    "        full_path = os.path.join(self.log_folder, path_log_file)\n",
    "\n",
    "        with open(full_path, \"a\") as file:\n",
    "\n",
    "            file.write(\"\\n===== New Episode =====\\n\")\n",
    "\n",
    "            for step in range(max_steps):\n",
    "\n",
    "                action = self.behavior_policy(current_state)\n",
    "                next_state = self._get_next_state(current_state, action)\n",
    "\n",
    "                reward = self.reward_matrix[next_state]\n",
    "\n",
    "                episode.append((current_state, action, reward))\n",
    "\n",
    "                # Log agent path\n",
    "                file.write(f\"Step {step+1}: \"\n",
    "                           f\"State={current_state}, \"\n",
    "                           f\"Action={action}, \"\n",
    "                           f\"Reward={reward}, \"\n",
    "                           f\"Next={next_state}\\n\")\n",
    "\n",
    "                if next_state == self.goal_state:\n",
    "                    file.write(\"Goal Reached.\\n\")\n",
    "                    break\n",
    "\n",
    "                current_state = next_state\n",
    "\n",
    "        return episode\n",
    "\n",
    "    # ===================================================\n",
    "    # Off-Policy Monte Carlo with Weighted Importance Sampling\n",
    "    # ===================================================\n",
    "    full_value_path = os.path.join(self.log_folder, value_log_file)\n",
    "    with open(full_value_path, \"w\") as file:\n",
    "\n",
    "        state_value_function = np.zeros((self.rows, self.cols))\n",
    "        cumulative_weights = np.zeros((self.rows, self.cols))\n",
    "\n",
    "        # Initialize target policy randomly\n",
    "        target_policy = {}\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.cols):\n",
    "                target_policy[(row, col)] = random.choice(self.action_space)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        with open(value_log_file, \"w\") as file:\n",
    "\n",
    "            for episode_index in range(number_of_episodes):\n",
    "\n",
    "                episode = self.generate_episode()\n",
    "\n",
    "                return_G = 0\n",
    "                importance_weight = 1\n",
    "\n",
    "                # Process episode backward\n",
    "                for time_step in reversed(range(len(episode))):\n",
    "\n",
    "                    state, action, reward = episode[time_step]\n",
    "                    row, col = state\n",
    "\n",
    "                    return_G = self.gamma * return_G + reward\n",
    "\n",
    "                    cumulative_weights[row, col] += importance_weight\n",
    "\n",
    "                    # Incremental update rule\n",
    "                    state_value_function[row, col] += (\n",
    "                        importance_weight / cumulative_weights[row, col]\n",
    "                    ) * (return_G - state_value_function[row, col])\n",
    "\n",
    "                    # Improve target policy greedily\n",
    "                    best_action = None\n",
    "                    best_value = -float(\"inf\")\n",
    "\n",
    "                    for possible_action in self.action_space:\n",
    "                        next_state = self._get_next_state(state,\n",
    "                                                          possible_action)\n",
    "                        next_row, next_col = next_state\n",
    "\n",
    "                        estimated_value = (\n",
    "                            self.reward_matrix[next_state] +\n",
    "                            self.gamma *\n",
    "                            state_value_function[next_row, next_col]\n",
    "                        )\n",
    "\n",
    "                        if estimated_value > best_value:\n",
    "                            best_value = estimated_value\n",
    "                            best_action = possible_action\n",
    "\n",
    "                    target_policy[state] = best_action\n",
    "\n",
    "                    # Stop if behavior deviates from target\n",
    "                    if action != target_policy[state]:\n",
    "                        break\n",
    "\n",
    "                    # Update importance sampling ratio\n",
    "                    importance_weight *= 1 / (1/4)\n",
    "\n",
    "                # Log value matrix every 500 episodes\n",
    "                if (episode_index + 1) % 500 == 0:\n",
    "                    file.write(f\"\\nEpisode {episode_index+1}\\n\")\n",
    "                    for row_values in state_value_function:\n",
    "                        file.write(\" \".join(\n",
    "                            f\"{value:7.2f}\" for value in row_values\n",
    "                        ) + \"\\n\")\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        return state_value_function, target_policy, \\\n",
    "               end_time - start_time\n",
    "\n",
    "\n",
    "def draw_grid_problem4(grid_object, title,\n",
    "                    value_function=None,\n",
    "                    policy=None):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for row in range(grid_object.rows):\n",
    "        for col in range(grid_object.cols):\n",
    "\n",
    "            color = \"white\"\n",
    "\n",
    "            if (row, col) in grid_object.grey_states:\n",
    "                color = \"darkgrey\"\n",
    "            elif (row, col) == grid_object.goal_state:\n",
    "                color = \"lightgreen\"\n",
    "\n",
    "            rectangle = patches.Rectangle(\n",
    "                (col, grid_object.rows - 1 - row),\n",
    "                1, 1,\n",
    "                edgecolor='black',\n",
    "                facecolor=color\n",
    "            )\n",
    "\n",
    "            ax.add_patch(rectangle)\n",
    "\n",
    "            # Display value function\n",
    "            if value_function is not None:\n",
    "                ax.text(col + 0.5,\n",
    "                        grid_object.rows - 1 - row + 0.5,\n",
    "                        round(value_function[row, col], 2),\n",
    "                        ha='center', va='center')\n",
    "\n",
    "            # Display policy arrows\n",
    "            if policy is not None:\n",
    "\n",
    "                if (row, col) not in grid_object.grey_states and \\\n",
    "                (row, col) != grid_object.goal_state:\n",
    "\n",
    "                    arrow_map = {\n",
    "                        \"up\": (0, 0.3),\n",
    "                        \"down\": (0, -0.3),\n",
    "                        \"left\": (-0.3, 0),\n",
    "                        \"right\": (0.3, 0)\n",
    "                    }\n",
    "\n",
    "                    dx, dy = arrow_map[policy[(row, col)]]\n",
    "\n",
    "                    ax.arrow(col + 0.5,\n",
    "                            grid_object.rows - 1 - row + 0.5,\n",
    "                            dx, dy,\n",
    "                            head_width=0.15,\n",
    "                            length_includes_head=True)\n",
    "\n",
    "    ax.set_xlim(0, grid_object.cols)\n",
    "    ax.set_ylim(0, grid_object.rows)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "grid_mc = GridWorld5x5MonteCarlo(\n",
    "    discount_factor=0.9,\n",
    "    log_folder=\"log\"\n",
    ")\n",
    "\n",
    "estimated_value_function, estimated_policy, mc_time = \\\n",
    "    grid_mc.off_policy_monte_carlo(number_of_episodes=5000)\n",
    "\n",
    "print(\"\\n===== MONTE CARLO ESTIMATED VALUE FUNCTION =====\")\n",
    "print(np.round(estimated_value_function, 2))\n",
    "print(\"Optimization Time:\", round(mc_time, 4), \"seconds\")\n",
    "\n",
    "# 1Initial Grid\n",
    "draw_grid_problem4(grid_mc, \"Problem 4 - Initial Grid\")\n",
    "\n",
    "# Monte Carlo Estimated Value Function\n",
    "draw_grid_problem4(grid_mc,\n",
    "                   \"Problem 4 - Monte Carlo Estimated V(s)\",\n",
    "                   value_function=estimated_value_function)\n",
    "\n",
    "# Monte Carlo Estimated Policy\n",
    "draw_grid_problem4(grid_mc,\n",
    "                   \"Problem 4 - Monte Carlo Estimated Policy\",\n",
    "                   policy=estimated_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c84de",
   "metadata": {},
   "source": [
    "## Comparison With Value Iteration\n",
    "\n",
    "| Aspect      | Value Iteration       | Monte Carlo                  |\n",
    "| ----------- | --------------------- | ---------------------------- |\n",
    "| Uses model? | Yes                   | No                           |\n",
    "| Speed       | Faster                | Slower                       |\n",
    "| Convergence | Deterministic         | High variance                |\n",
    "| Iterations  | ~10–20                | Thousands                    |\n",
    "| Complexity  | O(S × A × iterations) | O(episodes × episode_length) |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
